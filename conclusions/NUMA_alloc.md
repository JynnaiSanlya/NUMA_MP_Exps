- 申请到的内存地址所在的NUMA node应该是和目前所绑定的核相关。

- 在这个测试中，经过10次测试，12个线程申请到的node0和node1内存的比例为1:1。

- 在NUMA_alloc.c中，如果把前后的两个空for改为sleep(int n)，会出现奇怪的现象。直觉上来说，这个时候先到的线程会陷入休眠，并且让出处理器。所以node0的核心就有可能被让出来，并且优先被使用，导致申请到的内存大概率在node0（实测在size <= 263162的时候，20次测量12个线程中内存分配在node1的线程数不超过6，平均为3-4个，也即2:1）。然而实测size >= 263163的时候，结果突然完全倒了过来，在20次测量中分配在node1的平均有8个，不低于6。推测在通过malloc等方式分配内存的时候，存在某种阈值决定NUMA的分配优先级。

```bash
# 测量出263163这一阈值时的numastat
                           node0           node1
numa_hit               220082530       445456335
numa_miss                      0               0
numa_foreign                   0               0
interleave_hit              2558            2805
local_node             219015581       350071514
other_node               1066949        95384821
```